{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "This process relys heavily on requests and BeautifulSoup to crawl and scrape html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df \n",
    "from requests.exceptions import MissingSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "the GetValidUrl() Method pulls a websites url and varifies the sites validity\n",
    "When the application gets to a more self sustainable state this method can be implemented to validate given URL's for fast development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidUrl(): #in this method a user inputs a URL for validation\n",
    "    isVal = False\n",
    "    while isVal == False:   \n",
    "        try: \n",
    "            unVal = requests.get(input(\"Please enter an URL (includeHTTP://)':\")) #url requires http://\n",
    "            print(\"URL is Valid\")\n",
    "            isVal = True #ensures a url is valid for crawling/scraping\n",
    "            return unVal\n",
    "        except requests.ConnectionError as exception:\n",
    "            print(\"URL does not exist on Internet\") #if a correctly entered URL has no site\n",
    "        except MissingSchema as exception:\n",
    "            print(\"URL is not complete\") #This idicates a URL was missing schema needed for the method (http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "per_base() is a method that returns a dictionary with keys, but no values. This will be the basis for the persistant database we will keep event information in. Each key represents a collumn in a database. When parsing a webpage data will be stored in this same dictionary format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_base():\n",
    "    persistant_dic_base = {\n",
    "        'Event Title' :\"\",\n",
    "        'Topline': \"\",\n",
    "        'Headliner':\"\",\n",
    "        'Openers':\"\",\n",
    "        'Date(s)':\"\",\n",
    "        'Time(s)':\"\",\n",
    "        'Price/Admission':'',\n",
    "        'Age Restriction':'',\n",
    "        'Event Description':'',\n",
    "        'Venue':'',\n",
    "        'Venue Info':'',\n",
    "        'Street Name':'',\n",
    "        'Address Line 2':',',\n",
    "        'City':'',\n",
    "        'State':'',\n",
    "        'Postal Code':'',\n",
    "        'Event Image URL':\"\",\n",
    "        'Visit Event Website':'',\n",
    "        'Venue Info':'',\n",
    "        'Location Link':'',\n",
    "        'Get Tickets':'',\n",
    "        'URL':''}\n",
    "    return persistant_dic_base #this will be used as a template for dictionaries that will be used in the scrapign process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next two methods assist with web crawling\n",
    "\n",
    "webBuilder() is a method that takes a list of unparsed html code, and is specificly used for clensing data for Dada Dallas. \n",
    "\n",
    "Each item in the list of unparsed code contains a link to a specific events page within it. \n",
    "\n",
    "This method initiates a parsing process to find the a link in the html code, and appends it to a new list of links. \n",
    "\n",
    "It returns a list of links to event pages, for further crawling and scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webBuilder(page_list): #this method finds url's for events listed on the main callender page\n",
    "    events_list = []\n",
    "    for i in page_list:\n",
    "        cont = i.contents[0]\n",
    "        link = cont['href']\n",
    "        events_list.append(link)\n",
    "    return events_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visit_dallas_pages() is a method that takes an BeautifulSoup Element and a website URL. This method is specific to the Visit Dallas website.\n",
    "\n",
    "This method acomplishes the same task for webBuilder() but creates the list of unparsed html code containing links in the method. \n",
    "\n",
    "It takes a base URL beacuse the links that are parsed from the website contain the later half of the website. \n",
    "\n",
    "This method adds the bass URL to the parsed links to create a list of functional URL's that can be used for further parsing and scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visit_dallas_pages(souped, base):\n",
    "    pages=[]\n",
    "    base2 =\"\"\n",
    "    soupy = souped.find_all('a', class_=\"card event-card event-card--premiere\")\n",
    "    for i in soupy:\n",
    "        base2 = base + i.attrs['href']\n",
    "        pages.append(base2)\n",
    "    soupy = souped.find_all('a', class_= \"card event-card\")\n",
    "    for i in soupy:\n",
    "        base2 = base + i.attrs['href']\n",
    "        pages.append(base2)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer Notes: \n",
    "These two methods were constructed to help with the crawling/scraping of specific websites used in testing. In further iteration we plan on creating a single method that is able to crawl and return sites for scraping.\n",
    "\n",
    "\n",
    "\n",
    "### Scraping Methods\n",
    "\n",
    "dada_scrape() is a method that takes in a list of events and a dictionary. It ultimately returns a dataframe of with data from each page scraped. This method is specific to collecting data from the Dada Dallas website\n",
    "\n",
    "The dictionary is used as a template for how information from each page will be stored. \n",
    "\n",
    "For each page in the given list, it searcehs each page for various data, scrapes it, and adds it to a dictionary. \n",
    "\n",
    "At the end of scraping for a page, the filled out dictionary is added to a list. \n",
    "\n",
    "At the end of the method, the list of dictionaries is converted into a dataframe that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dada_scrape(cal_events, pbas=dict):\n",
    "    page_deets = []\n",
    "    for link in cal_events:\n",
    "        cont_list = pbas.copy()\n",
    "        on_sale = True\n",
    "        s_page = requests.get(link)\n",
    "        sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "        sub_parse = sub_soup.find('meta', property =\"og:title\")\n",
    "        cont_list.update([('Event Title', sub_parse.attrs['content'])])\n",
    "        sub_parse = sub_soup.find('section', class_=\"topline-info presented-by\")\n",
    "        cont_list.update([('Topline', sub_parse.get_text())])\n",
    "        sub_parse = sub_soup.find('h1', class_=\"event-name headliners\")\n",
    "        cont_list.update([('Headliner', sub_parse.get_text())])\n",
    "        sub_parse = sub_soup.find('h2', class_=\"supports\")\n",
    "        if type(sub_parse) is type(None):\n",
    "            cont_list.update([('Openers', 'NA')])\n",
    "        else:\n",
    "            cont_list.update([('Openers', sub_parse.get_text())])\n",
    "        sub_parse = sub_soup.find('span', class_=\"dates\")\n",
    "        cont_list.update([('Date(s)', sub_parse.get_text())])\n",
    "        sub_parse = sub_soup.find('span', class_=\"start\")\n",
    "        cont_list.update([('Time(s)', sub_parse.get_text())])\n",
    "        sub_parse = sub_soup.find('span', class_=\"sales-ended inactive\")\n",
    "        if type(sub_parse) is type(None):\n",
    "            sub_parse = sub_soup.find('span', class_=\"price-range\")\n",
    "            cont_list.update([('Price/Admission', sub_parse.get_text())])\n",
    "        else:\n",
    "            cont_list.update([('Price/Admission', sub_parse.get_text())])\n",
    "            on_sale = False\n",
    "        sub_parse = sub_soup.find('section', class_=\"age-restriction all-ages\")\n",
    "        cont_list.update([('Age Restriction', sub_parse.get_text())])\n",
    "        sub_parse = sub_soup.find('article', class_=\"event-description\")\n",
    "        cont_list.update([('Event Description', sub_parse.contents[1].text + \" \" +sub_parse.contents[3].text)])\n",
    "        sub_parse = sub_soup.find('address', class_=\"venue-info\")\n",
    "        cont_list.update([('Venue', sub_parse.contents[2])])\n",
    "        sub_parse = sub_soup.find('meta', property=\"og:street-address\")\n",
    "        cont_list.update([('Street Name', sub_parse.attrs['content'])])\n",
    "        sub_parse = sub_soup.find('meta', property=\"og:locality\")\n",
    "        cont_list.update([('City', sub_parse.attrs['content'])])\n",
    "        sub_parse = sub_soup.find('meta', property=\"og:region\")\n",
    "        cont_list.update([('State', sub_parse.attrs['content'])])\n",
    "        sub_parse = sub_soup.find('meta', property=\"og:postal-code\")\n",
    "        cont_list.update([('Postal Code', sub_parse.attrs['content'])])\n",
    "        sub_parse = sub_soup.find('img')\n",
    "        cont_list.update([('Event Image URL', sub_parse.attrs['src'])])\n",
    "        cont_list.update([('URL',link)])\n",
    "        \n",
    "        if on_sale == True:\n",
    "            sub_parse = sub_soup.find('section', class_ = 'ticket-price')\n",
    "            for i in sub_parse.contents:\n",
    "                if type(i) == bs4.element.Tag and 'href' in i.attrs:\n",
    "                    cont_list.update([('Get Tickets', i.attrs['href'])])\n",
    "                elif type(i) == bs4.element.Tag:\n",
    "                        for j in i.contents:\n",
    "                            if type(j)== bs4.element.Tag and 'href' in j.attrs: \n",
    "                                cont_list.update([('Get Tickets', j.attrs['href'])])\n",
    "        else:\n",
    "            cont_list.update([('Get Tickets', cont_list['Price/Admission'])])\n",
    "            \n",
    "            \n",
    "        page_deets.append(cont_list)\n",
    "        \n",
    "    spp = df.from_records(page_deets)\n",
    "    return spp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vd_scraper() is a method that takes in a list of events, a bass url, and a dictionary. This scrapign method is speciclly for the Visit Dallas Website\n",
    "\n",
    "The dictionary is used as a template for how information from each page will be stored. \n",
    "\n",
    "For each page in the given list, it searcehs each page for various data, scrapes it, and adds it to a dictionary. \n",
    "\n",
    "At the end of scraping for a page, the filled out dictionary is added to a list. \n",
    "\n",
    "At the end of the method, the list of dictionaries is converted into a dataframe that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vd_scraper(vd_events, base, pbas=dict):\n",
    "    page_deets = []\n",
    "    for link in vd_events:\n",
    "        pg_con = pbas.copy()\n",
    "        s_page = requests.get(link)\n",
    "        sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "        sub_parse = sub_soup.find('h1', class_=\"event-perma-card__title\")\n",
    "        pg_con.update([('Event Title', sub_parse.get_text())])\n",
    "        sub_parse = sub_soup.find('div', class_=\"event-perma-card__date-tag\")\n",
    "        span =\"\"\n",
    "        for i in sub_parse.contents:   \n",
    "            if i != '\\n':\n",
    "                if span ==\"\":\n",
    "                    span = i.get_text()\n",
    "                else:\n",
    "                    span = span + \" \" + i.get_text()\n",
    "        pg_con.update([('Date(s)', span)])\n",
    "        sub_parse = sub_soup.find_all('p', class_=\"event-perma-content__text\")\n",
    "        #print(sub_parse[1].contents[1].get_text())\n",
    "        pg_con.update([('Venue', sub_parse[1].contents[1].get_text())])\n",
    "        pg_con.update([('Street Name', sub_parse[1].contents[4])])\n",
    "        if len(sub_parse[1].contents) > 10:\n",
    "            pg_con.update([(\"Address Line 2\", sub_parse[1].contents[6])])\n",
    "            ay = sub_parse[1].contents[8]\n",
    "            pg_con.update([('City', ay.split()[0])])\n",
    "            pg_con.update([('State',ay.split()[1])])\n",
    "            pg_con.update([('Postal Code',ay.split()[2])])\n",
    "        else:\n",
    "            pg_con.update([(\"Address Line 2\", \"NA\")])\n",
    "            ay = sub_parse[1].contents[6]\n",
    "            pg_con.update([('City', ay.split()[0])])\n",
    "            pg_con.update([('State',ay.split()[1])])\n",
    "            pg_con.update([('Postal Code',ay.split()[2])])\n",
    "        oi = sub_parse[2].contents[0]\n",
    "        oii = oi.split(\"  \")\n",
    "        pg_con.update([('Time(s)', oii[len(oii)-1])])\n",
    "        if len(sub_parse) > 3:\n",
    "            pg_con.update([('Price/Admission', sub_parse[3].contents[0])])\n",
    "        else:\n",
    "            pg_con.update([('Price/Admission', \"\")])\n",
    "        sub_parse = sub_soup.find('div', class_='wysiwyg')\n",
    "        pg_con.update([('Event Description', sub_parse.get_text()[1:])])\n",
    "        sub_parse = sub_soup.find('div', class_='event-perma-card__meta-ctas-wrap')\n",
    "        pg_con.update([('Get Tickets',\"\")])\n",
    "        pg_con.update([('Visit Event Website',\"\")])\n",
    "        pg_con.update([('Venue Info',\"\")])\n",
    "        for i in sub_parse.contents:\n",
    "            if type(i) == bs4.element.Tag:\n",
    "                for j in i.contents:\n",
    "                    if type(j) == bs4.element.Tag:\n",
    "                        pg_con.update([(j.get_text(),(base + j['href']))])\n",
    "        sub_parse = sub_soup.find('p', class_='event-perma-card__meta-item pre-icon--pin') \n",
    "        if type(sub_parse) == bs4.element.Tag:\n",
    "            pg_con.update([('Location Link', sub_parse.contents[0]['href'])])\n",
    "        sub_parse = sub_soup.find('div', class_='event-perma-card__image')\n",
    "        pg_con.update([('Event Image URL', sub_parse.contents[1]['src'])])\n",
    "        \n",
    "        pg_con.update([('URL',link)])        \n",
    "        page_deets.append(pg_con)\n",
    "    spp = df.from_records(page_deets)    \n",
    "    return spp    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer Notes: \n",
    "These two methods were constructed to help with the scraping of specific websites used in testing. In further development this process is aimed to be streamlined to more flexibly integrate with sites out there.\n",
    "\n",
    "\n",
    "### Crawling and Scraping\n",
    "\n",
    "In this milestone we look at two websites that contain information about events in Dallas: Dadadallas.com and visitdallas.com. \n",
    "\n",
    "Club Dada , also known as Dada Dallas, is a popular venue and in Deep Ellum. VisitDallas is an independent, not-for-profit organization that promotes Dallas as a business and pleasure destination. \n",
    "\n",
    "Both of these sites have pages of events, and in this milestone we developed a web scraper that can collect information from these websites.\n",
    "\n",
    "\n",
    "\n",
    "In this first cell we scrape data for Dada Dallas events. It takes the webpage and parses it using beautiful soup and the methods created earlier. It finds all the active calendar events, collects their URL's, and then scrapes each event page for information. Each pages information is stored in a dictionary, added to a list of dictionaries, which is converted into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = requests.get(\"https://www.dadadallas.com/calendar/\")\n",
    "site.encoding = 'ISO-885901'\n",
    "soup = BeautifulSoup(site.text, 'html.parser')\n",
    "page_list = soup.find_all(class_='event-name headliners')\n",
    "sub_pages = webBuilder(page_list)\n",
    "pbas = per_base()\n",
    "j1 = dada_scrape(sub_pages, pbas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next call we scrape data from the VisitDallas website. This functions similary to the previous cell, but uses its own case specific methods for scraping.\n",
    "\n",
    "At the end of the cell the scraping method returns a dataframe with information from all events scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "site2 = requests.get(\"https://www.visitdallas.com/things-to-do/events/arts-culture-in-dallas/index.html\")\n",
    "site2.encoding = 'ISO-885901'\n",
    "soup2 = BeautifulSoup(site2.text, 'html.parser')\n",
    "su_parse = soup2.find_all('a', class_= \"card event-card\")\n",
    "base = \"https://www.visitdallas.com\"\n",
    "dal_list = visit_dallas_pages(soup2, base)\n",
    "pbas = per_base()\n",
    "j2 = vd_scraper(dal_list, base, pbas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two dataframes with event data from different websites. \n",
    "\n",
    "Because we used the same dictionary format to save data for these events, the two dataframes can easily be appended into a larger persistant database.\n",
    "\n",
    "Once we have this centralized database, it can be exported into a csv for further clensing/use/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "j3 = j1.append(j2)\n",
    "df.to_csv(j3, \"PDBASE.csv\", sep = \",\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
