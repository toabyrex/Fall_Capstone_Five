{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3\n",
    "## Group 5\n",
    "\n",
    "\n",
    "Python code for the Artsonepass Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame as df \n",
    "from requests.exceptions import MissingSchema\n",
    "import gspread \n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from df2gspread import df2gspread as d2g "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawly:\n",
    "    def __init__(self, key, url):\n",
    "        self.key = key\n",
    "        self.url = url\n",
    "        self.sub_pg = []\n",
    "    \n",
    "    def crawl(self): #Spider crawls pages to collect list of URLs\n",
    "        if self.key == '{:04d}'.format(0): #Dada Dallas\n",
    "            self.sub_pg=[]\n",
    "            site = requests.get(self.url)\n",
    "            site.encoding = 'ISO-885901'\n",
    "            soup = BeautifulSoup(site.text, 'html.parser')\n",
    "            page_list = soup.find_all(class_='event-name headliners')\n",
    "            for i in page_list:\n",
    "                cont = i.contents[0]\n",
    "                link = cont['href']\n",
    "                self.sub_pg.append(link)\n",
    "        elif self.key =='{:04d}'.format(1):#texas ballet theater \n",
    "            self.sub_pg=[]\n",
    "            site = requests.get(self.url)\n",
    "            site.encoding = 'ISO-885901'\n",
    "            soup = BeautifulSoup(site.text, 'html.parser')\n",
    "            page_list = soup.find_all(\"div\", class_=\"bottom_btns\")\n",
    "            for i in page_list:\n",
    "                cont = i.contents[1]\n",
    "                self.sub_pg.append(i.contents[1]['href'])\n",
    "        elif self.key =='{:04d}'.format(2):  #Theater3\n",
    "            self.sub_pg =[]\n",
    "            site = requests.get(self.url)\n",
    "            site.encoding = 'ISO-885901'\n",
    "            soup = BeautifulSoup(site.text, 'lxml')\n",
    "            self.sub_pg = soup.find_all(\"a\", class_=\"staff__item staff__item--five-columns\")\n",
    "        elif self.key =='{:04d}'.format(3):#Latino Cultural Center\n",
    "            self.sub_pg=[]\n",
    "            site = requests.get(self.url)\n",
    "            site.encoding = 'ISO-885901'\n",
    "            soup = BeautifulSoup(site.text, 'html.parser')\n",
    "            page_list = soup.find(\"iframe\", class_=\"iframe-class\")\n",
    "            site = requests.get(page_list.get('src'))\n",
    "            site.encoding = 'ISO-885901'\n",
    "            soup = BeautifulSoup(site.text, 'lxml')\n",
    "            sub_par = soup.find('div', class_='aswidget')\n",
    "            for i in sub_par.find_all(\"a\"):\n",
    "                self.sub_pg.append(\"http:\"+i.get(\"href\"))\n",
    "        elif self.key =='{:04d}'.format(5):#Trees\n",
    "            self.sub_pg=[]\n",
    "            site=requests.get(self.url)\n",
    "            soup = BeautifulSoup(site.text,\"html.parser\")\n",
    "            for i in soup.find_all(\"div\",class_=\"thumb\"):\n",
    "                self.sub_pg.append(i.a.get(\"href\"))\n",
    "        elif self.key =='{:04d}'.format(6):#Dallas Arboretum\n",
    "            self.sub_pg=[]\n",
    "            site=requests.get(self.url)\n",
    "            soup = BeautifulSoup(site.text,\"html.parser\")\n",
    "            for i in soup.find_all(\"div\",class_=\"eventCard__links\"):\n",
    "                self.sub_pg.append(i.a.get(\"href\"))    \n",
    "            self.sub_pg = list(dict.fromkeys(self.sub_pg))\n",
    "        elif self.key =='{:04d}'.format(7):#6floor museum,\n",
    "            self.sub_pg=[]\n",
    "            site=requests.get(self.url)\n",
    "            soup = BeautifulSoup(site.text,\"lxml\")\n",
    "            for i in soup.find_all(\"h3\",class_=\"tribe-events-month-event-title entry-title summary\"):\n",
    "                self.sub_pg.append(i.contents[4].get(\"href\"))    \n",
    "            self.sub_pg = list(dict.fromkeys(self.sub_pg))\n",
    "        else:\n",
    "            print(\"Invalid Key\")\n",
    "            return[]\n",
    "        return self.sub_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "class creepy: #Scrapes crawled pages\n",
    "    prd = {  #Dictionary Keys for formating\n",
    "        'Org Key':\"\",\n",
    "        'Event Key':\"\",\n",
    "        'Event Title' :\"\",\n",
    "        'Topline': \"\",\n",
    "        'Headliner':\"\",\n",
    "        'Openers':\"\",\n",
    "        'Date(s)':\"\",\n",
    "        'Time(s)':\"\",\n",
    "        'Price/Admission':'',\n",
    "        'Age Restriction':'',\n",
    "        'Event Description':'',\n",
    "        'Staff/Artists':'',\n",
    "        'Category':'',\n",
    "        'Venue':'',\n",
    "        'Venue Info':'',\n",
    "        'Street Name':'',\n",
    "        'Address Line 2':',',\n",
    "        'City':'',\n",
    "        'State':'',\n",
    "        'Postal Code':'',\n",
    "        'Event Image URL':\"\",\n",
    "        'Venue Info':'',\n",
    "        'Location Link':'',\n",
    "        'Get Tickets':'',\n",
    "        'URL':''}\n",
    "    \n",
    "    def __init__(self, key, pages):\n",
    "        self.key = key#ID\n",
    "        self.pages = pages#pages to scrape\n",
    "        self.info = []\n",
    "        self.count=0\n",
    "        \n",
    "    def creep(self, bonus=str):\n",
    "        if self.key == '{:04d}'.format(0): #Dada Dallas\n",
    "            for link in self.pages:\n",
    "                event_info = creepy.prd.copy()\n",
    "                on_sale = True\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                s_page = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "                sub_parse = sub_soup.find('meta', property =\"og:title\")\n",
    "                event_info.update([('Event Title', sub_parse.attrs['content'])])\n",
    "                sub_parse = sub_soup.find('section', class_=\"topline-info presented-by\")\n",
    "                if type(sub_parse) is type(None):\n",
    "                    event_info.update([('Topline', \"NA\")])\n",
    "                else:    \n",
    "                    event_info.update([('Topline', sub_parse.get_text())])\n",
    "                sub_parse = sub_soup.find('h1', class_=\"event-name headliners\")\n",
    "                event_info.update([('Headliner', sub_parse.get_text())])\n",
    "                sub_parse = sub_soup.find('h2', class_=\"supports\")\n",
    "                if type(sub_parse) is type(None):\n",
    "                    event_info.update([('Openers', 'NA')])\n",
    "                else:\n",
    "                    event_info.update([('Openers', sub_parse.get_text())])\n",
    "                sub_parse = sub_soup.find('span', class_=\"dates\")\n",
    "                event_info.update([('Date(s)', sub_parse.get_text())])\n",
    "                sub_parse = sub_soup.find('span', class_=\"start\")\n",
    "                event_info.update([('Time(s)', sub_parse.get_text())])\n",
    "                sub_parse = sub_soup.find('span', class_=\"sales-ended inactive\")\n",
    "                if type(sub_parse) is type(None):\n",
    "                    sub_parse = sub_soup.find('span', class_=\"price-range\")\n",
    "                    event_info.update([('Price/Admission', sub_parse.get_text())])\n",
    "                else:\n",
    "                    event_info.update([('Price/Admission', sub_parse.get_text())])\n",
    "                    on_sale = False\n",
    "                sub_parse = sub_soup.find('section', class_=\"age-restriction all-ages\")\n",
    "                if type(sub_parse) is type(None):\n",
    "                    sub_parse = sub_soup.find('section', class_=\"age-restriction over-21\")\n",
    "                    event_info.update([('Age Restriction', sub_parse.get_text())])\n",
    "                else:    \n",
    "                    event_info.update([('Age Restriction', sub_parse.get_text())])\n",
    "                sub_parse = sub_soup.find('article', class_=\"event-description\")\n",
    "                event_info.update([('Event Description', sub_parse.contents[1].text + \" \" +sub_parse.contents[3].text)])\n",
    "                event_info.update([('Category', \"Performance\")])\n",
    "                sub_parse = sub_soup.find('address', class_=\"venue-info\")\n",
    "                event_info.update([('Venue', sub_parse.contents[2])])\n",
    "                event_info.update([('Venue Info', \"Night Club\")])\n",
    "                sub_parse = sub_soup.find('meta', property=\"og:street-address\")\n",
    "                event_info.update([('Street Name', sub_parse.attrs['content'])])\n",
    "                sub_parse = sub_soup.find('meta', property=\"og:locality\")\n",
    "                event_info.update([('City', sub_parse.attrs['content'])])\n",
    "                sub_parse = sub_soup.find('meta', property=\"og:region\")\n",
    "                event_info.update([('State', sub_parse.attrs['content'])])\n",
    "                sub_parse = sub_soup.find('meta', property=\"og:postal-code\")\n",
    "                event_info.update([('Postal Code', sub_parse.attrs['content'])])\n",
    "                sub_parse = sub_soup.find('img')\n",
    "                event_info.update([('Event Image URL', sub_parse.attrs['src'])])\n",
    "                event_info.update([('URL',link)])\n",
    "        \n",
    "                if on_sale == True:\n",
    "                    sub_parse = sub_soup.find('section', class_ = 'ticket-price')\n",
    "                    for i in sub_parse.contents:\n",
    "                        if type(i) == bs4.element.Tag and 'href' in i.attrs:\n",
    "                            event_info.update([('Get Tickets', i.attrs['href'])])\n",
    "                        elif type(i) == bs4.element.Tag:\n",
    "                            for j in i.contents:\n",
    "                                if type(j)== bs4.element.Tag and 'href' in j.attrs: \n",
    "                                    event_info.update([('Get Tickets', j.attrs['href'])])\n",
    "                else:\n",
    "                    event_info.update([('Get Tickets', event_info['Price/Admission'])])\n",
    "                event_info.update([('Location Link', 'https://goo.gl/maps/d3G9381S4j1zD73s9')])\n",
    "            \n",
    "                self.info.append(event_info) \n",
    "            return self.info\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif self.key == '{:04d}'.format(1): #texas ballet\n",
    "            for link in self.pages:\n",
    "                event_info = self.prd.copy()\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                s_page = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "                sub_par = sub_soup.find('div', class_='title')\n",
    "                vTitle = (sub_par.h1.text.replace('\\n','') if sub_par.h1 else contents[0]).strip()\n",
    "                event_info.update([(\"Event Title\",vTitle)])\n",
    "                sub_par = sub_soup.find('div', class_='additional_info')\n",
    "                vDates =(sub_par.p.text.replace('\\n','') if sub_par.p else contents[0]).strip()\n",
    "                event_info.update([(\"Date(s)\",vDates)])\n",
    "                sub_par = sub_soup.find(\"div\", class_='main_title')\n",
    "                desc=[]\n",
    "                desc.append((sub_par.h3.text if sub_par.h3 else contetns[0]).strip())\n",
    "                sub_par = sub_soup.find(\"div\", class_='main_info')\n",
    "                desc.append((sub_par.p.text if sub_par.p else contetns[0]).strip())\n",
    "                sub_par = sub_soup.find_all(\"div\", class_='single_info')\n",
    "                for i in range(len(sub_par)):\n",
    "                    desc.append((sub_par[i].h4.text.replace('\\n','') if sub_par[i].h4 else contents[0]).strip())\n",
    "                    desc.append(sub_par[i].p.text)\n",
    "                sub_par = sub_soup.find_all(\"div\", class_='additional_info')\n",
    "                vdes = (sub_par[1].text.replace('\\n','') if sub_par[1].text else contents[0]).strip()\n",
    "                desc.append(vdes)\n",
    "                des=''\n",
    "                event_info.update([(\"Topline\", desc.pop(0))])\n",
    "                for i in desc:\n",
    "                    if i == desc[len(desc)-1]:\n",
    "                        event_info.update([(\"Venue\",i)])\n",
    "                    else: \n",
    "                        des+= i\n",
    "                        des+=\"\\n\\n\"\n",
    "                event_info.update([(\"Event Description\",des)])\n",
    "                event_info.update([('Venue Info', \"Performance Hall\")])\n",
    "                event_info.update([('Category', \"Performance\")])\n",
    "                sub_par = sub_soup.find_all(\"div\", class_='single_person')\n",
    "                ar=[]\n",
    "                for i in range(len(sub_par)):\n",
    "                    ar.append((sub_par[i].text.replace('\\n',\"\") if sub_par[0].text else contents[0]).strip())\n",
    "                art=\"\"\n",
    "                for i in ar:\n",
    "                    art += i\n",
    "                    art += \"\\n\\n\"\n",
    "                event_info.update([(\"Staff/Artists\",art)])\n",
    "                sub_par = sub_soup.find(class_='image_holder_single')\n",
    "                event_info.update([('Event Image URL', sub_par.contents[1].get('src'))])\n",
    "                event_info.update([('URL',link)])\n",
    "                \n",
    "                #\n",
    "                self.info.append(event_info)      \n",
    "               \n",
    "            spec = requests.get(\"https://texasballettheater.org/special-events/\")\n",
    "            spec.encoding = 'ISO-885901'\n",
    "            sub_soup = BeautifulSoup(spec.text, 'lxml')\n",
    "            page_list = sub_soup.find_all(\"div\", class_=\"item same_height\")\n",
    "            bal_list = []\n",
    "            for i in page_list:\n",
    "                cont = i.contents[1]\n",
    "                bal_list.append(cont['href'])\n",
    "                \n",
    "            for link in bal_list: #special events\n",
    "                event_info = self.prd.copy()\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                s_page = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "                sub_par = sub_soup.find('div', class_='title')\n",
    "                vTitle = (sub_par.h1.text.replace('\\n','') if sub_par.h1 else contents[0]).strip()\n",
    "                event_info.update([(\"Event Title\",vTitle)])\n",
    "                sub_par = sub_soup.find('section', class_='post_image')\n",
    "                event_info.update([(\"Event Image URL\",sub_par.contents[1].get(\"src\"))])\n",
    "                event_info.update([('Category', \"Visual,\")])\n",
    "                sub_par = sub_soup.find_all('div', class_='col-md-3 col-sm-6 col-12')\n",
    "                dinfo = (sub_par[0].text.replace('  ',\"\").replace('\\n',' ') if sub_par[0].text else contents[0]).strip()\n",
    "                event_info.update([(\"Date(s)\", dinfo)])\n",
    "                sub_par.pop(0)\n",
    "                tinfo = (sub_par[0].text.replace('  ',\"\").replace('\\n',' ') if sub_par[0].text else contents[0]).strip()\n",
    "                event_info.update([(\"Time(s)\", tinfo)])\n",
    "                sub_par.pop(0)\n",
    "                vinfo = (sub_par[0].text.replace('  ',\"\").replace('\\n',' ') if sub_par[0].text else contents[0]).strip()\n",
    "                event_info.update([(\"Venue\", vinfo)])\n",
    "                event_info.update([('Venue Info', \"Night Club\")])\n",
    "                sub_par.pop(0)\n",
    "                pinfo = (sub_par[0].text.replace('  ',\"\").replace('\\n',' ') if sub_par[0].text else contents[0]).strip()\n",
    "                event_info.update([(\"Price/Admission\", pinfo)])\n",
    "                sub_par.pop(0)\n",
    "                sub_par = sub_soup.find('section', class_='content_text')\n",
    "                event_info.update([('Event Description', sub_par.text.replace(\"\\n\",\" \").replace(\"\\xa0\",\"\"))])\n",
    "                event_info.update([('URL',link)])\n",
    "                sub_par = sub_soup.find('a', class_='dark_btn')\n",
    "                event_info.update([('Get Tickets',sub_par.get(\"href\"))])\n",
    "                \n",
    "                \n",
    "                self.info.append(event_info) \n",
    "            return self.info\n",
    "        elif self.key == '{:04d}'.format(2):\n",
    "            for i in self.pages:\n",
    "                event_info = self.prd.copy()\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                event_info.update([('Event Image URL', i.contents[1].contents[1].get(\"data-src\"))])\n",
    "                \n",
    "                \n",
    "                \n",
    "                if len(i.contents[3])<3 :\n",
    "                    event_info.update([('Event Title',\"NA\")]) #title \n",
    "                    event_info.update([('Event Description',\"Event data TBD\")])\n",
    "                elif type(i.contents[3].contents[1].h5) == bs4.element.Tag:\n",
    "                    event_info.update([('Event Title',i.contents[3].contents[1].h5.text)]) #title\n",
    "                    l =i.contents[3].find_all('p')\n",
    "                    event_info.update([('Topline',l[0].text)]) #topline\n",
    "                    event_info.update([('Event Description',l[1].text)]) # desc\n",
    "                    event_info.update([('Time(s)',l[2].text)]) #time\n",
    "                elif type(i.contents[3].p.text.split(\"\\n\",1)[0]) == str:\n",
    "                    event_info.update([(\"Event Title\",i.contents[3].p.text.split(\"\\n\",1)[0])])\n",
    "                    event_info.update([(\"Event Description\",i.contents[3].p.text.split(\"\\n\",1)[1])])\n",
    "                    event_info.update([(\"Time(s)\",i.contents[3].find_all('p')[1].text)])\n",
    "               \n",
    "                event_info.update([('Category', \"Performance\")])\n",
    "\n",
    "                event_info.update([('URL', bonus)])\n",
    "                self.info.append(event_info)\n",
    "            \n",
    "            return self.info\n",
    "        \n",
    "        \n",
    "        elif self.key == '{:04d}'.format(3):#Latino Cultural Center\n",
    "            for link in self.pages:\n",
    "                event_info = self.prd.copy()\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                s_page = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "                event_info.update([(\"Event Title\",sub_soup.find('h1', class_='title').text)])\n",
    "                event_info.update([(\"Event Image URL\",(\"http:\"+str(sub_soup.find(\"span\",class_=\"header_thumbnail\").contents[1]).split(\"\\\"\",2)[1]))])\n",
    "                event_info.update([(\"URL\",link)])\n",
    "                event_info.update([(\"Venue\",sub_soup.find('span', class_='meta').text)])\n",
    "                event_info.update([(\"Event Description\",sub_soup.find(\"div\",class_=\"entry-content\").p.contents[0].get(\"content\").replace(\"\\r\",\"\").replace(\"\\xa0\",\"\").replace(\"\\xa0A\",\"\").replace(\"\\n\",\" \"))])\n",
    "                \n",
    "                \n",
    "                self.info.append(event_info)\n",
    "            return self.info\n",
    "        \n",
    "        \n",
    "        elif self.key == '{:04d}'.format(5):# Trees\n",
    "            for link in self.pages:\n",
    "                event_info = self.prd.copy()\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Category',\"Performance\")])\n",
    "                event_info.update([('Venue Info',\"Night Club\")])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                event_info.update([(\"URL\",link)])\n",
    "                s_page = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "                event_info.update([(\"Event Title\",sub_soup.find(\"div\",class_=\"page_header_left\").h1.text)])    \n",
    "                event_info.update([(\"Openers\", sub_soup.find(\"div\", class_=\"page_header_left\").h4.text.replace(\"\\n\",\" - \"))])\n",
    "                event_info.update([(\"Get Tickets\",sub_soup.find(\"div\", class_=\"page_header_right\").a.get(\"href\"))])\n",
    "                event_info.update([(\"Event Image URL\",sub_soup.find(\"div\", class_=\"content_item event_image gutter-bottom\").img.get(\"src\"))])\n",
    "                sub_par = sub_soup.find(\"ul\",class_=\"details\")\n",
    "                event_info.update([(\"Date(s)\",sub_par.contents[1].span.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\"))])\n",
    "                if len(sub_par.find_all(\"span\")) >4:\n",
    "                    time = \"Start time: {}. Doors: {}\".format(sub_par.find_all(\"span\")[2].text,sub_par.find_all(\"span\")[4].text)\n",
    "                elif len(sub_par.find_all(\"span\"))>2:\n",
    "                    time = \"Start Time: {}\".format(sub_par.find_all(\"span\")[2].text)\n",
    "                event_info.update([(\"Time(s)\", time)])\n",
    "                if type(sub_soup.find(\"span\", class_=\"age_res\")) != type(None):\n",
    "                    event_info.update([(\"Age Restriction\", sub_soup.find(\"span\", class_=\"age_res\").text)])\n",
    "                if type(sub_soup.find(\"div\", class_=\"collapse-wrapper\")) != type(None):\n",
    "                    event_info.update([(\"Event Description\",sub_soup.find(\"div\", class_=\"collapse-wrapper\").text.replace(\"\\n\",\" \"))])\n",
    "                vinfo=sub_soup.find(\"div\", class_=\"venueinfo\").text.replace(\"\\n\",\"\").replace(\"\\t\",\" \").split(\"       \")\n",
    "                for i in range(len(vinfo)):\n",
    "                    vinfo[i] = vinfo[i].replace(\"  \",\"\")\n",
    "                event_info.update([(\"Venue\",vinfo[0])])\n",
    "                event_info.update([(\"Street Name\",vinfo[1])])\n",
    "                event_info.update([(\"City\",vinfo[2].split()[0])])\n",
    "                event_info.update([(\"State\",vinfo[2].split()[1])])\n",
    "                event_info.update([(\"Postal Code\",vinfo[2].split()[2])])\n",
    "                self.info.append(event_info)\n",
    "            return self.info\n",
    "        elif self.key == '{:04d}'.format(6): #Dallas Arboretum\n",
    "            for link in self.pages:\n",
    "                event_info = self.prd.copy()\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Category',\"Science and Nature\")])\n",
    "                event_info.update([('Venue Info',\"Botanical Garden\")])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                event_info.update([(\"URL\",link)])\n",
    "                s_page = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "                sub_par=sub_soup.find(\"div\", class_=\"centered__card\")\n",
    "                event_info.update([(\"Event Title\",sub_par.h1.text.replace(\"\\n\",\"\").strip())])\n",
    "                event_info.update([(\"Street Name\",\"8525 Garland Road\")])\n",
    "                event_info.update([(\"City\",\"Dallas\")])\n",
    "                event_info.update([(\"State\",\"TX\")])\n",
    "                event_info.update([(\"Postal Code\",\"75218\")])\n",
    "                sub_par=sub_soup.find_all(\"div\", class_=\"wp-block-column\") \n",
    "                event_info.update([(\"Event Image URL\", sub_par[0].find('img').get(\"data-src\"))])\n",
    "                sub_par= sub_par[1].find_all('p')\n",
    "                for i in range(len(sub_par)):\n",
    "                    if sub_par[i].text == \"Venue: \":\n",
    "                        event_info.update([(\"Venue\",\"{} @ The Dallas Arboretum\".format(sub_par[i+1].text))])\n",
    "                    elif sub_par[i].text == \"Pricing: \":\n",
    "                        event_info.update([(\"Price/Admission\",sub_par[i+1].text)])\n",
    "                    elif sub_par[i].text == \"Description: \":\n",
    "                        event_info.update([(\"Event Description\", sub_par[i+1].text)])\n",
    "                sub_par=sub_soup.find_all(\"div\", class_=\"wp-block-columns has-4-columns spacing__mtn\")\n",
    "                dates = []\n",
    "                times=[]\n",
    "                for i in range(len(sub_par)):\n",
    "                    for j in sub_par[i].find_all(\"span\",class_=\"styles__event-date\"):\n",
    "                        dates.append(j.text.strip())\n",
    "                for i in range(len(sub_par)):\n",
    "                    for j in sub_par[i].find_all(\"span\",class_=\"styles__event-time\"):\n",
    "                        times.append(j.text.strip())\n",
    "                event_info.update([(\"Date(s)\", dates)])\n",
    "                event_info.update([(\"Time(s)\", times)])\n",
    "                tl = []\n",
    "                for i in sub_soup.find_all(\"a\", class_=\"button__primary button__emphasis button__icon\"):\n",
    "                    tl.append(i.get(\"href\"))\n",
    "                for i in sub_soup.find_all(\"a\", class_=\"button__primary button__emphasis button__icon button__small\"):\n",
    "                    tl.append(i.get(\"href\"))\n",
    "                event_info.update([(\"Get Tickets\", tl)])\n",
    "                self.info.append(event_info)\n",
    "            return self.info\n",
    "        \n",
    "        \n",
    "        elif self.key == '{:04d}'.format(7):#6 floor museum \n",
    "            for link in self.pages:\n",
    "                event_info = self.prd.copy()\n",
    "                event_info.update([('Org Key',self.key)])\n",
    "                event_info.update([('Category',\"History\")])\n",
    "                event_info.update([('Venue Info',\"Historic Museum\")])\n",
    "                event_info.update([('Event Key','{:06d}'.format(self.count))])\n",
    "                self.count+=1\n",
    "                event_info.update([(\"URL\",link)])\n",
    "                s_page = requests.get(link)\n",
    "                sub_soup = BeautifulSoup(s_page.text, 'html.parser')\n",
    "                if type(sub_soup.find(\"div\", class_=\"centered__card\")) != type(None):\n",
    "                    event_info.update([(\"Event Title\",sub_soup.find(\"div\", class_=\"centered__card\").h1.text.replace(\"\\n\",\"\").strip())])\n",
    "                elif(type(sub_soup.find(\"div\", class_=\"tribe-events-single\")) != type(None)):\n",
    "                    event_info.update([(\"Event Title\",sub_soup.find(\"div\", class_=\"tribe-events-single\").h1.text.replace(\"\\n\",\"\").strip())])\n",
    "                dates=[]\n",
    "                for i in sub_soup.find_all(\"abbr\"):\n",
    "                    if len(i.text.strip()) >4:\n",
    "                        dates.append(i.text.strip())\n",
    "                event_info.update([(\"Date(s)\", dates)])\n",
    "                event_info.update([(\"Time(s)\", sub_soup.find(\"div\", class_=\"tribe-events-schedule tribe-clearfix\").h2.text)])\n",
    "                event_info.update([(\"Price/Admission\", sub_soup.find(\"span\", class_=\"tribe-events-cost\").text)])\n",
    "                event_info.update([(\"Event Description\", sub_soup.find(\"div\", class_=\"tribe-events-single-event-description tribe-events-content\").text.replace(\"\\n\",\" \"))])\n",
    "                event_info.update([(\"Venue\", sub_soup.find(\"dd\",class_=\"tribe-venue\").text.strip())])\n",
    "                for i in event_info[\"Venue\"].split():\n",
    "                    if str(i) == \"Online\" or str(i)==\"Virtual\" or str(i) == \"online\" or str(i)==\"virtual\":\n",
    "                        event_info.update([(\"Venue Info\", \"Virtual\")])\n",
    "                if event_info[\"Venue Info\"] != \"Virtual\":\n",
    "                    event_info.update([(\"Street Name\", sub_soup.find(\"span\", class_=\"tribe-street-address\").text)])\n",
    "                    event_info.update([(\"City\", sub_soup.find(\"span\", class_=\"tribe-locality\").text)])\n",
    "                    event_info.update([(\"State\", sub_soup.find(\"abbr\", class_=\"tribe-region tribe-events-abbr\").text)])\n",
    "                    event_info.update([(\"Postal Code\", sub_soup.find(\"span\", class_=\"tribe-postal-code\").text)])\n",
    "                    event_info.update([(\"Location Link\", sub_soup.find(\"a\",class_=\"tribe-events-gmap\").get(\"href\"))])\n",
    "                self.info.append(event_info)\n",
    "            return self.info\n",
    "        else:\n",
    "            print(\"Invalid Key\")\n",
    "            return[]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Affiliate Organization class is the primary class for scraping data and manging organizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "class af_org:\n",
    "    \n",
    "    id_count=0\n",
    "    orgs ={}                                              #List of Orgs\n",
    "    org_data =pd.DataFrame()                            #Scraped Data\n",
    "    flag_data = pd.DataFrame()                            #Flagged Data\n",
    "    send_data = pd.DataFrame()                            #Akkio prediction input data\n",
    "    pre_data = pd.DataFrame()                            #Akkio Prediction Values\n",
    "    par_AK = {\"flow_key\":\"<FlowKey>\", \"api_key\":\"<ApiKey>\", \"data\":\"\"}\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def to_local(cls):                                                        #to Local Machine CSV\n",
    "        df.to_csv(af_org.org_data, \"M3.csv\", sep = \",\", index = False)\n",
    "        df.to_csv(af_org.flag_data, \"F3.csv\", sep = \",\", index = False)\n",
    "        df.to_csv(af_org.pre_data, \"P3.csv\", sep = \",\", index = False)\n",
    "    @classmethod\n",
    "    def akkio_run(cls):                                   #Akkil AI Predicts values for events with missing categories\n",
    "        af_org.send_data = af_org.org_data.copy()\n",
    "        af_org.send_data= af_org.send_data.reset_index().drop(columns = [\"index\",\"Org Key\",\"Event Key\",\"Street Name\",\"Address Line 2\",\"City\",\"State\",\"Postal Code\",\"Event Image URL\",\"Location Link\",\"Get Tickets\",\"URL\"])\n",
    "   \n",
    "        da=[]\n",
    "        pred=[] \n",
    "        tpos =-1\n",
    "        for i in range(len(af_org.send_data.index)):\n",
    "            temp={}\n",
    "    \n",
    "            if af_org.send_data.loc[i,\"Age Restriction\"] == \"\" or af_org.send_data.loc[i,\"Category\"] == \"\" or af_org.send_data.loc[i,\"Venue Info\"] == \"\":\n",
    "\n",
    "                for j in range(len(ajj.columns)):\n",
    "                    if af_org.send_data.keys()[j] != \"Age Restriction\" and af_org.send_data.keys()[j] != \"Category\" and af_org.send_data.keys()[j] != \"Venue Info\":\n",
    "                        if af_org.send_data.iat[i,j] != \"\" and type(af_org.send_data.iat[i,j])!=list:\n",
    "                            temp.update([(af_org.send_data.keys()[j],af_org.send_data.iat[i,j])])\n",
    "                        elif af_org.send_data.iat[i,j] != \"\" and type(af_org.send_data.iat[i,j])==list and len(af_org.send_data.iat[i,j]) != 0:\n",
    "                            temp.update([(af_org.send_data.keys()[j],af_org.send_data.iat[i,j][0])])\n",
    "                da.append(temp)\n",
    "                tpos+=1\n",
    "                ev=[]\n",
    "                ev.append(da[tpos])\n",
    "                ev = json.dumps(ev, indent=4)\n",
    "                af_org.par_AK.update([(\"data\",ev)])\n",
    "                ml = requests.get(\"https://api.akk.io/api\", params= af_org.par_AK).json()#Collect prediction values\n",
    "                pred.append([af_org.send_data.index[i],ml])\n",
    "        pre_par = []\n",
    "        for i in pred:\n",
    "            l={}\n",
    "            l.update([(\"Index\",i[0])])\n",
    "            l.update([(\"Org Key\",af_org.org_data.iloc[i[0],0])])\n",
    "            l.update([(\"Event Key\",af_org.org_data.iloc[i[0],1])])\n",
    "            l.update([(\"Event Title\",af_org.org_data.iloc[i[0],2])])\n",
    "            for j in i[1]:\n",
    "                for y in j.items():\n",
    "                    if y[0]==\"Age Restriction\" or y[0]==\"Category\" or y[0]==\"Venue Info\":\n",
    "                        l.update([(\"PV: {}\".format(y[0]),y[1])])\n",
    "                    else:\n",
    "                        cut = y[0].split()\n",
    "                        short=\"\"\n",
    "                        for i in cut[cut.index(\"is\"):]:\n",
    "                            short = short+\" \"+i\n",
    "                        l.update([(short,y[1])])\n",
    "            pre_par.append(l)\n",
    "        af_org.pre_data = df.from_records(pre_par)\n",
    "    \n",
    "    @classmethod\n",
    "    def g_run(cls):                                   #Sends parsed data to Google Sheets Cloud\n",
    "        scope = ['https://spreadsheets.google.com/feeds'] \n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name('<Service Account Credentials File>', scope) \n",
    "        gc = gspread.authorize(credentials)\n",
    "        d2g.upload(af_org.org_data, \"<Sheets ID>\", \"Affiliate Organizations\", credentials=credentials, row_names=True)\n",
    "        d2g.upload(af_org.flag_data, \"<Sheets ID>\", \"Flagged Affiliate Organizations\", credentials=credentials, row_names=True)\n",
    "        d2g.upload(af_org.pre_data, \"<Sheets ID>\", \"Akkio Predictions\", credentials=credentials, row_names=True)\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def g_get(cls):                                   #Pulls data from a Google Sheets Cloud\n",
    "        scope = ['https://spreadsheets.google.com/feeds'] \n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name('<Service Account Credentials File>', scope) \n",
    "        gc = gspread.authorize(credentials)\n",
    "        spreadsheet_key = 'sheets key' \n",
    "        book = gc.open_by_key(spreadsheet_key) \n",
    "        worksheet = book.worksheet(\"Akkio Predictions\") \n",
    "        table = worksheet.get_all_values()\n",
    "        stat = df.from_records(table).drop(0, axis = 1)\n",
    "        stem = stat.iloc[0]\n",
    "        stat = stat[1:]\n",
    "        stat.columns = stem\n",
    "        return(stat)\n",
    "        \n",
    "        \n",
    "    def __init__(self, name):                                                           # Initialize Event Org\n",
    "        self.name = name                                                           #Org Name \n",
    "        self.sUrl =\"\"                                                               #URL to crawl\n",
    "        self.ssurl = \"\"                                                           # alternate url to crawl\n",
    "        self.ID = '{:04d}'.format(af_org.id_count)                                  #org ID \n",
    "        af_org.id_count+=1                                                           \n",
    "        self.sub_pg = []                                                           #pages to scrape \n",
    "        self.data = []                                                           #event data \n",
    "        self.info= {\"Org ID\": self.ID,                                                           #info \n",
    "                    \"Org Name\": self.name,\n",
    "                    \"Org URL\": self.sUrl}\n",
    "        af_org.orgs[self.ID] = self.name                                                            \n",
    "        \n",
    "    def set_sUrl(self, sUrl):                                                           # set URL\n",
    "        try: \n",
    "            requests.get(sUrl) \n",
    "        except InvalidSchema as exception:\n",
    "            print(\"URL is not complete: Please try again\")\n",
    "        except requests.ConnectionError as exception:\n",
    "            print(\"URL does not exist on Internet. Please try Annother URL\") \n",
    "        except MissingSchema as exception:\n",
    "            print(\"URL is not complete: Please try again\") \n",
    "        self.sUrl = sUrl\n",
    "        self.info['Org URL'] = sUrl\n",
    "    \n",
    "    def update(self, data):                                 #new data is added and sorted in the master dataframe\n",
    "        self.data = data\n",
    "        for i in range(len(af_org.org_data)):\n",
    "            if af_org.org_data.at[i,\"Org Key\"]=='{:04d}'.format(int(self.ID)):\n",
    "                af_org.org_data = af_org.org_data.drop(i)\n",
    "        af_org.org_data = af_org.org_data.append(df.from_records(self.data), ignore_index = True)\n",
    "        af_org.org_data = af_org.org_data.sort_values(by=['Org Key',\"Event Key\"])\n",
    "        \n",
    "        flags = af_org.org_data.copy()\n",
    "        flags = flags.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        flags = flags.fillna(\"FLAG\")\n",
    "        for i in range(len(flags.columns)):             #Data is parsed and empty cells are flagged\n",
    "            if flags.columns[i] == \"Category\":\n",
    "                for j in range(len(flags[flags.columns[i]])):\n",
    "                    if flags.iat[j,i] == \"FLAG\":\n",
    "                        flags.iat[j,i] +=\": CATEGORY\"\n",
    "            elif flags.columns[i] == \"Age Restriction\":\n",
    "                for j in range(len(flags[flags.columns[i]])):   #Important informatrion have alternate flag tags\n",
    "                    if flags.iat[j,i] == \"FLAG\":\n",
    "                        flags.iat[j,i] +=\": AGE\"\n",
    "            elif flags.columns[i] == \"Event Image URL\":\n",
    "                for j in range(len(flags[flags.columns[i]])):\n",
    "                    if flags.iat[j,i] == \"FLAG\":\n",
    "                        flags.iat[j,i] +=\": IMAGE\"\n",
    "            elif flags.columns[i] == \"Venue\":\n",
    "                for j in range(len(flags[flags.columns[i]])):\n",
    "                    if flags.iat[j,i] == \"FLAG\":\n",
    "                        flags.iat[j,i] +=\": VENUE\"\n",
    "            elif flags.columns[i] == \"Event Title\":\n",
    "                for j in range(len(flags[flags.columns[i]])):\n",
    "                    if flags.iat[j,i] == \"FLAG\":\n",
    "                        flags.iat[j,i] +=\": TITLE\"\n",
    "            elif flags.columns[i] == \"Event Description\":\n",
    "                for j in range(len(flags[flags.columns[i]])):\n",
    "                    if flags.iat[j,i] == \"FLAG\":\n",
    "                        flags.iat[j,i] +=\": DESCRIPTION\"\n",
    "        af_org.flag_data = flags\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running crawl and scrape functions\n",
    "\n",
    "This creates the organization classes and runs the scraping and crawling methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "dada = af_org(\"Dada Dallas\")\n",
    "dada.set_sUrl(\"https://www.dadadallas.com/calendar/\")\n",
    "dada.sub_pg = crawly(dada.ID, dada.sUrl).crawl()\n",
    "dada.update(creepy(dada.ID, dada.sub_pg).creep())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbt = af_org(\"Texas Ballet Theater\")\n",
    "tbt.set_sUrl('https://texasballettheater.org/20-21season/')\n",
    "tbt.ssUrl = \"https://texasballettheater.org/special-events/\" #Special secconday page \n",
    "tbt.sub_pg = crawly(tbt.ID, tbt.sUrl).crawl()\n",
    "tbt.update(creepy(tbt.ID, tbt.sub_pg).creep())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "the = af_org(\"Theater3\")\n",
    "the.set_sUrl(\"https://www.theatre3dallas.com/shows-tickets/\")\n",
    "the.ID = '{:04d}'.format(2)\n",
    "the.sub_pg = crawly(the.ID, the.sUrl).crawl()\n",
    "the.update(creepy(the.ID, the.sub_pg).creep(the.sUrl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCC = af_org(\"Latino Cultural Center\")\n",
    "LCC.ID = '{:04d}'.format(3)\n",
    "LCC.set_sUrl(\"http://lcc.dallasculture.org/programs/event-calendar/\")\n",
    "LCC.sub_pg = crawly(LCC.ID, LCC.sUrl).crawl()\n",
    "LCC.update(creepy(LCC.ID, LCC.sub_pg).creep())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tre = af_org(\"Trees\")\n",
    "tre.ID = '{:04d}'.format(5)\n",
    "tre.set_sUrl(\"https://www.treesdallas.com/events/all\")\n",
    "tre.sub_pg = crawly(tre.ID, tre.sUrl).crawl()\n",
    "tre.update(creepy(tre.ID,tre.sub_pg).creep())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb = af_org(\"The Dallas Arboretum\")\n",
    "arb.ID = '{:04d}'.format(6)\n",
    "arb.set_sUrl(\"https://www.dallasarboretum.org/events-activities/calendar/\")\n",
    "arb.sub_pg = crawly(arb.ID, arb.sUrl).crawl()\n",
    "arb.update(creepy(arb.ID, arb.sub_pg).creep())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfk = af_org(\"6 floor museum\")\n",
    "jfk.ID = '{:04d}'.format(7)\n",
    "jfk.set_sUrl(\"https://www.jfk.org/events/\")\n",
    "jfk.sub_pg = crawly(jfk.ID, jfk.sUrl).crawl()\n",
    "jfk.update(creepy(jfk.ID, jfk.sub_pg).creep())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect To the Cloud\n",
    "\n",
    "Run the class methods that sends data to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "#af_org.akkio_run()                             #Low on Akkio Resources\n",
    "af_org.pre_data = af_org.g_get().copy()                                         #Pulls previous ^ data from gsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_org.g_run()                                         #Sends scraped data to gsheets cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_org.to_local()                                         #saves scraped data to Machine as CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
